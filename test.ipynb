{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruvsrikanth/Work/Projects/MPCS/Quarter_4/Fundamentals of Deep Learning/Project/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import src\n",
    "from src.utils import create_gif, load_model\n",
    "from torchvision.transforms import Compose, Lambda, ToPILImage\n",
    "import numpy as np\n",
    "import torch\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_start = 0.0001\n",
    "beta_end = 0.02\n",
    "timesteps = 200\n",
    "image_size = 28\n",
    "num_channels = 1\n",
    "batch_size = 128\n",
    "device = \"cpu\"\n",
    "epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = src.LinearScheduler(beta_start=beta_start, beta_end=beta_end, timesteps=timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_transform = Compose([\n",
    "     Lambda(lambda t: (t + 1) / 2),\n",
    "     Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
    "     Lambda(lambda t: t * 255.),\n",
    "     Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "     ToPILImage(),\n",
    "])\n",
    "\n",
    "forward_diffusion = src.ForwardDiffusion(sqrt_alphas_cumprod=scheduler.sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod=scheduler.sqrt_one_minus_alphas_cumprod, reverse_transform=reverse_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = src.DDPM(n_features=image_size, in_channels=num_channels, channel_scale_factors=(1, 2, 4,))\n",
    "load_model(epoch=epoch, model=model, device=device)\n",
    "\n",
    "sampler = src.Sampler(betas=scheduler.betas, sqrt_one_minus_alphas_cumprod=scheduler.sqrt_one_minus_alphas_cumprod, sqrt_one_by_alphas=scheduler.sqrt_one_by_alphas, posterior_variance=scheduler.posterior_variance, timesteps=timesteps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get samples\n",
    "samples = sampler.sample(model=model, image_size=image_size, batch_size=batch_size, channels=num_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tensor or list of tensors expected, got <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m create_gif(samples\u001b[39m=\u001b[39;49msamples, image_size\u001b[39m=\u001b[39;49mimage_size, num_channels\u001b[39m=\u001b[39;49mnum_channels, timesteps\u001b[39m=\u001b[39;49mtimesteps, n_images\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n",
      "File \u001b[0;32m~/Work/Projects/MPCS/Quarter_4/Fundamentals of Deep Learning/Project/src/utils/utils.py:77\u001b[0m, in \u001b[0;36mcreate_gif\u001b[0;34m(samples, n_images, image_size, num_channels, timesteps)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(timesteps):\n\u001b[1;32m     76\u001b[0m     n_square_images \u001b[39m=\u001b[39m samples[i][:n_images\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mreshape(n_images\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m, image_size, image_size, num_channels)\n\u001b[0;32m---> 77\u001b[0m     image_grid \u001b[39m=\u001b[39m make_grid(n_square_images, nrow\u001b[39m=\u001b[39;49mn_images)\n\u001b[1;32m     78\u001b[0m     img_grid \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mimshow(image_grid, cmap\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m\"\u001b[39m, animated\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     79\u001b[0m     img_grids\u001b[39m.\u001b[39mappend([img_grid])\n",
      "File \u001b[0;32m~/Work/Projects/MPCS/Quarter_4/Fundamentals of Deep Learning/Project/.venv/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Work/Projects/MPCS/Quarter_4/Fundamentals of Deep Learning/Project/.venv/lib/python3.9/site-packages/torchvision/utils.py:63\u001b[0m, in \u001b[0;36mmake_grid\u001b[0;34m(tensor, nrow, padding, normalize, value_range, scale_each, pad_value, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensor or list of tensors expected, got a list containing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(t)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensor or list of tensors expected, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[39m# if list of tensors, convert to a 4D mini-batch Tensor\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, \u001b[39mlist\u001b[39m):\n",
      "\u001b[0;31mTypeError\u001b[0m: tensor or list of tensors expected, got <class 'numpy.ndarray'>"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_gif(samples=samples, image_size=image_size, num_channels=num_channels, timesteps=timesteps, n_images=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8706a86bad710adf2f66a3e7a8383362f29f6e7e4ec4a73c68d06803313a6507"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
